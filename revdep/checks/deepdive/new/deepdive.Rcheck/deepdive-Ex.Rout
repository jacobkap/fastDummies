
R version 4.3.1 (2023-06-16 ucrt) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "deepdive"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> options(pager = "console")
> library('deepdive')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("deepforest")
> ### * deepforest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deepforest
> ### Title: Build or train bagged deeptree or deepnet of multiple
> ###   architecture
> ### Aliases: deepforest
> 
> ### ** Examples
> 
> 
> require(deepdive)
> 
> x<-data.frame(x1=runif(10),x2=runif(10))
> y<-data.frame(y=10*x$x1+20*x$x2+20)
> 
> mdeepf<-deepforest(x,y,
+                   networkCount=2,
+                   layerChoice=c(2:3),
+                   unitsChoice=c(4:10),
+                   cutVarSizePercent=0.6,
+                   cutDataSizePercent=0.6,
+                   activation = c('relu',"sin"),
+                   reluLeak=0.01,
+                   modelType ='regress',
+                   iterations = 10,
+                   eta = 10 ^-2,
+                   seed=2,
+                   gradientClip=0.8,
+                   regularisePar=0,
+                   optimiser="adam",
+                   parMomentum=0.9,
+                   inputSizeImpact=1,
+                   parRmsPropZeroAdjust=10^-8,
+                   parRmsProp=0.9999,
+                   treeLeaves=NA,
+                   treeMinSplitPercent=0.3,
+                   treeMinSplitCount=100,
+                   treeCp=0.01 ,
+                   errorCover=0.2,
+                   treeAugment=TRUE,
+                   printItrSize=100,
+                   showProgress=TRUE,
+                   stopError=0.01,
+                   miniBatchSize=64,
+                   useBatchProgress=TRUE)
iteration 1: 9.24429810186586
iteration 3: 6.78685740726008
iteration 5: 8.92847455356447
iteration 7: 6.95718817074448
iteration 10: 5.13276354532344
iteration 1: 13.1824145809719
iteration 3: 8.29651336683338
iteration 5: 4.99680861081992
iteration 7: 6.63883933526789
iteration 10: 7.03205204046531
> 
> 
> 
> cleanEx()
> nameEx("deepnet")
> ### * deepnet
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deepnet
> ### Title: Build and train an Artificial Neural Network of any size
> ### Aliases: deepnet
> 
> ### ** Examples
> 
> require(deepdive)
> 
> x <- data.frame(x1 = runif(10),x2 = runif(10))
> y<- data.frame(y=20*x$x1 +30*x$x2+10)
> 
> #train
> modelnet<-deepnet(x,y,c(2,2),
+ activation = c('relu',"sigmoid"),
+ reluLeak = 0.01,
+ modelType = "regress",
+ iterations =5,
+ eta=0.8,
+ optimiser="adam")
iteration 0: 56.5339796456271
iteration 1: 9.61797635396268
iteration 2: 24.6173673449624
iteration 3: 27.8009474828186
iteration 5: 21.5190937980461
> 
> #predict
> predDeepNet<-predict.deepnet(modelnet,newData=x)
> 
> #evaluate
> sqrt(mean((predDeepNet$ypred-y$y)^2))
[1] 21.51909
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("deeptree")
> ### * deeptree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: deeptree
> ### Title: Descision Tree augmented by Artificial Neural Network
> ### Aliases: deeptree
> 
> ### ** Examples
> 
> 
> require(deepdive)
> 
> x <- data.frame(x1 = runif(10),x2 = runif(10))
> 
> y<- data.frame(y=20*x$x1 +30* x$x2 +10)
> 
> deepTreeMod<-deeptree(x,
+ y,
+ hiddenLayerUnits=c(4,4),
+ activation = c('relu',"sin"),
+ reluLeak=0.01,
+ modelType ='regress',
+ iterations = 1000,
+ eta = 0.4,
+ seed=2,
+ gradientClip=0.8,
+ regularisePar=0,
+ optimiser="adam",
+ parMomentum=0.9,
+ inputSizeImpact=1,
+ parRmsPropZeroAdjust=10^-8,
+ parRmsProp=0.9999,
+ treeLeaves=NA,
+ treeMinSplitPercent=0.4,
+ treeMinSplitCount=100,
+ stackPred =NA,
+ stopError=4,
+ miniBatchSize=64,
+ useBatchProgress=TRUE,
+ ignoreNAerror=FALSE)
iteration 99: 6.40365040763981
iteration 199: 5.91979095325606
iteration 299: 6.20103600534354
iteration 399: 7.48260076792978
iteration 499: 4.9801907407987
iteration 599: 5.16361482380621
iteration 699: 1.70864189860278
iteration 700: 1.70864189860278
Reached stop error. Reduce Change or set StopError to NA if would like more iterations 
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  0.63 0.04 0.67 NA NA 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
